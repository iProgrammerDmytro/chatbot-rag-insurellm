# InsureLLM QA Chatbot (FastAPI + Chroma RAG)

RAG over Markdown:

- Vector DB: Chroma (HNSW)
- Embeddings: sentence-transformers
- API: FastAPI
- LLM: OpenAI (graceful fallback to “I don’t know.” without key)

## Features

- Ingest Markdown from `knowledge-base/` with stable IDs and optional namespace filter
- Fast retrieval via Chroma + sentence-transformers
- Async OpenAI completions with low temperature
- Clean separation: tiny endpoint, service layer, simple CLI for ingest

## Requirements

- Python 3.11
- Optional: `.env` with `OPENAI_API_KEY=<your-key>` (for LLM answers)

## Install & Run (local)

```bash
python -m venv .venv && source .venv/bin/activate
pip install -r requirements.txt

# Ingest Markdown into Chroma (drop existing)
python cli.py load --drop

# Start API
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

Health:

```bash
curl http://127.0.0.1:8000/health
```

## API

- POST `/api/chatbot/ask`

Request:

```json
{
  "question": "What does the company do?",
  "k": 5,
  "ns": null,
  "include_passages": false
}
```

Response (shape):

```json
{
  "answer": "…",
  "sources": [{ "title": "…", "source": "company/about.md", "distance": 0.12 }],
  "used": {
    "db": "chroma-db",
    "collection": "knowledge-base",
    "embedding_model": "sentence-transformers/all-MiniLM-L6-v2",
    "llm_model": "gpt-4o-mini",
    "k": 5,
    "ns": null
  },
  "passages": null
}
```

## CLI

```bash
# Re/ingest all Markdown
python cli.py load --drop

# Only a namespace (e.g., `company`)
python cli.py load --ns company
```

Defaults come from `app/core/config.py` (overridable via env):

- `KB_ROOT=knowledge-base`
- `CHROMA_DIR=chroma-db`
- `CHROMA_COLLECTION=knowledge-base`
- `EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2`
- `OPENAI_MODEL=gpt-4o-mini`
- `OPENAI_API_KEY=<your-key>`

## Docker (local)

```bash
docker compose up --build
# then (optional re/ingest inside container):
docker exec -it chatbot-api python cli.py load --drop
```

Tip: If you prefer auto‑ingest at container start without shell access, you can set the container command to run:

```bash
python cli.py load --drop && uvicorn app.main:app --host 0.0.0.0 --port 8000
```

(or wire a tiny startup hook that calls `Loader(...)()` once; uses existing Loader).

## Repo structure

- `app/main.py`: FastAPI app setup
- `app/chatbot/`: request/response schemas, router, and service
- `app/vector_db/loader.py`: Markdown → Chroma ingestion
- `app/vector_db/retriever.py`: Chroma retrieval
- `app/openai/llm.py`: async OpenAI client
- `cli.py`: ingestion CLI

## Notes

- Without `OPENAI_API_KEY`, answers gracefully fallback to “I don’t know.”
- Keep `chroma-db/` out of Git; it’s regenerated by ingest.
- Model cache issues in containers: ensure writable cache directories or redirect HF/Transformers caches to `/tmp`.
